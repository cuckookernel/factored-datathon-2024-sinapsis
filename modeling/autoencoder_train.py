"""Training of autoencoder"""
from dataclasses import dataclass
from importlib import reload

import torch as pt
from sentence_transformers import SentenceTransformer
from torch import Tensor, nn, optim
from torch.utils.data import DataLoader

import modeling.autoencoder_arch as arch
from data_proc.common import gdelt_base_data_path
from data_proc.nl_util import compute_svp_descriptions
from modeling.parquet_dataset import ParquetFilesDataset
from modeling.torch_trainer import Trainer
from shared import date, logging, pd, runpyfile, timedelta
from shared.databricks_conn import Connection

L = logging.getLogger("ae_train")
# %%

@dataclass
class HyperParams:
    """Training hyper-params"""

    layer_dims: list[int]
    batch_size: int = 64
    shuffle_dataset: bool = False
    num_epochs: int = 10
    lr: float = 1e-3


def _interactive_testing() -> None:
    # TODO: reinclud in ruff checks (drop from ruff.toml[exclude])
    # _download_data_for_autoencoder_training(db_conn,
    #                                        start_date=date(2023, 8, 13),
    #                                        end_date=date.today(),
    #                                        days_batch=10)
    # %%

    # TODO (Mateo): do we need normalization?
    # transform = transforms.Compose([
    #    transforms.Normalize((0.5,), (0.5,))
    # ])
    runpyfile("modeling/autoencoder_train.py")
    reload(arch)
    # %%
    train_dataset = ParquetFilesDataset(
        files_dir=gdelt_base_data_path(),
        # TODO (Mateo): make blob cover everything!
        file_name_pattern="svp_descs_embeddings_2023-*.parquet",
        feature_cols = ["svp_embedding"],
        target_col = None,
        preload=False,
        # TODO (Mateo): remove truncation,
        truncate_size=100,
    )

    valid_dataset = ParquetFilesDataset(
        files_dir=gdelt_base_data_path(),
        # TODO (Mateo): make blob cover everything!
        file_name_pattern="svp_descs_embeddings_2024-01-1*.parquet",
        feature_cols=["svp_embedding"],
        target_col=None,
        preload=False,
    )


    input_dim = train_dataset[0]["svp_embedding"].shape[0]
    L.info(f"Embeddings dim={input_dim}")
    # %%
    reload(arch)

    hpars = HyperParams(layer_dims=[input_dim, 350, 350, 350],
                        num_epochs=40,
                        lr=1e-6,
                        batch_size=64)
    # %%

    n_epochs_log = 1

    train_dloader = DataLoader(train_dataset,
                              batch_size=hpars.batch_size,
                              shuffle=hpars.shuffle_dataset)
    valid_dloader = DataLoader(valid_dataset,
                              batch_size=hpars.batch_size,
                              shuffle=hpars.shuffle_dataset)

    model = run_training(hpars, train_dloader, valid_dloader,
                         n_epochs_log=n_epochs_log)
    # %%
    # for batch in train_dloader:
    for batch in valid_dloader:
        in_embed = batch['svp_embedding'].to("cuda")
        break

    out_embed = model(in_embed)
    pt.norm(out_embed, dim=1)

    loss_fn = nn.MSELoss()
    # batch_size = in_embed.shape[0]
    print(f"loss_fn {loss_fn(in_embed, out_embed).item():.6f}",
          f"norm-diff: {pt.norm(in_embed - out_embed, dim=1).mean().item():.6f}")

    pt.sum((in_embed - out_embed) ** 2, dim=1).mean().item() / 384
    # %%

def run_training(hpars: HyperParams,
                 train_dloader: DataLoader,
                 valid_dloader: DataLoader,
                 n_epochs_log: int = 5) -> arch.Autoencoder:
    """Train the autoencoder"""
    # %%

    model = arch.Autoencoder(hpars.layer_dims)
    print(model, flush=True)

    loss_fn = nn.MSELoss(reduction="none")
    # def loss_fn(pred: Tensor, target: Tensor) -> Tensor:
    #    return pt.sum((pred - target)**2)

    optimizer = optim.Adam(model.parameters(), lr=hpars.lr)

    trainer = Trainer(model,
                      batch_proc_fn=autoenc_batch_post_proc,
                      loss_fn=loss_fn,
                      optimizer=optimizer)

    return trainer.fit(train_dloader, valid_dloader,
                       num_epochs=hpars.num_epochs,
                       n_epochs_log=n_epochs_log)
    # %%


def autoenc_batch_post_proc(batch: dict[str, Tensor]) -> tuple[Tensor, Tensor]:
    """Extract input (X) and output (y) from the batch generated by dataloader"""
    return  batch["svp_embedding"], batch["svp_embedding"]


def _download_data_for_autoencoder_training(db_conn: Connection,
                                            start_date: date,
                                            end_date: date,
                                            days_batch: int = 10) -> None:
    """Download events enriched generate svp descriptions and save to local"""
    n_intervs = (end_date - start_date).days / days_batch + 1
    dates = [start_date + i * timedelta(days=10)  for i in range(n_intervs)]

    embedder = SentenceTransformer("all-MiniLM-L6-v2")

    for i in range(len(dates) - 1):
        date1 = dates[i]
        date2 = dates[i+1]
        L.info(f"date1={date1!r} date2={date2!r} querying data bricks")
        events_df = pd.read_sql(
            f"""SELECT *
            FROM gdelt.events_enriched
            TABLESAMPLE (1 PERCENT) REPEATABLE (0)
            where date_added >= '{date1}'
            and date_added <= '{date2}'
            """, # noqa: S608
            con = db_conn )

        L.info(f"date1={date1!r} date2={date2!r} computing descriptions")
        svp_descs_df = compute_svp_descriptions(events_df)
        L.info(f"date1={date1!r} date2={date2!r} computing embeddings")
        embeddings = embedder.encode(svp_descs_df['svp_desc'])
        svp_descs_df['svp_embedding'] = list(embeddings)
        out_fpath = gdelt_base_data_path() / f'svp_descs_embeddings_{date1}_{date2}_1pct.parquet'
        svp_descs_df.to_parquet(out_fpath)
        L.info(f"date1={date1!r} date2={date2!r} "
               f"saved to parquet={out_fpath} size={out_fpath.lstat().st_size/1e6:.2f} MB")
    # %%
